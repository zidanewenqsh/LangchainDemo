import os
from dotenv import load_dotenv
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡µé¢é…ç½®
st.set_page_config(page_title="AI åŠ©æ‰‹", page_icon="ğŸ¤–")
st.title("AI åŠ©æ‰‹")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ– ChatOpenAI
llm = ChatOpenAI(
    model="deepseek-chat",  # ä½¿ç”¨ DeepSeek æ¨¡å‹
    temperature=0.7,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_API_BASE")
)

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
    ("human", "{input}")
])

# åˆ›å»ºå¤„ç†é“¾
chain = prompt | llm | StrOutputParser()

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # æ·»åŠ åŠ©æ‰‹å“åº”
    with st.chat_message("assistant"):
        response = chain.invoke({"input": prompt})
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response}) 
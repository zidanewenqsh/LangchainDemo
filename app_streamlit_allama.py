import os
from dotenv import load_dotenv
import streamlit as st
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import logging
import sys
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('streamlit_app.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# é…ç½® requests çš„é‡è¯•ç­–ç•¥
session = requests.Session()
retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
)
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)

def check_ollama_service():
    """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
    try:
        response = session.get(
            "http://localhost:11434/api/version",
            timeout=5
        )
        is_running = response.status_code == 200
        logger.info(f"Ollama æœåŠ¡çŠ¶æ€æ£€æŸ¥: {'è¿è¡Œä¸­' if is_running else 'æœªè¿è¡Œ'}")
        return is_running
    except Exception as e:
        logger.error(f"æ£€æŸ¥ Ollama æœåŠ¡æ—¶å‡ºé”™: {str(e)}")
        return False

def check_model_availability():
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        response = session.get(
            "http://localhost:11434/api/tags",
            timeout=5
        )
        if response.status_code == 200:
            models = response.json()
            logger.info(f"å¯ç”¨æ¨¡å‹åˆ—è¡¨: {models}")
            # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹åç§°å˜ä½“
            model_names = [model['name'].lower() for model in models.get('models', [])]
            logger.info(f"æ£€æµ‹åˆ°çš„æ¨¡å‹: {model_names}")
            return any(name for name in model_names if 'qwen' in name)
        return False
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§æ—¶å‡ºé”™: {str(e)}")
        return False

def get_available_model_name():
    """è·å–å¯ç”¨çš„æ¨¡å‹åç§°"""
    try:
        response = session.get(
            "http://localhost:11434/api/tags",
            timeout=5
        )
        if response.status_code == 200:
            models = response.json().get('models', [])
            # æŸ¥æ‰¾åŒ…å« qwen çš„æ¨¡å‹åç§°
            for model in models:
                name = model['name'].lower()
                if 'qwen' in name:
                    logger.info(f"æ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹: {model['name']}")
                    return model['name']
        logger.warning("æœªæ‰¾åˆ° Qwen ç›¸å…³æ¨¡å‹")
        return None
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åç§°æ—¶å‡ºé”™: {str(e)}")
        return None

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
logger.info("ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Ollama AI åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide"
)
st.title("Ollama AI åŠ©æ‰‹")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
    logger.info("åˆå§‹åŒ–ä¼šè¯çŠ¶æ€")

# åœ¨åˆå§‹åŒ– Ollama ä¹‹å‰æ·»åŠ æ£€æŸ¥
if not check_ollama_service():
    st.error("Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆè¿è¡Œ 'ollama serve' å‘½ä»¤å¯åŠ¨æœåŠ¡")
    logger.error("Ollama æœåŠ¡æœªè¿è¡Œ")
    st.stop()

# è·å–å®é™…å¯ç”¨çš„æ¨¡å‹åç§°
model_name = get_available_model_name()
if not model_name:
    st.error("æœªæ‰¾åˆ° Qwen ç›¸å…³æ¨¡å‹ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ­£ç¡®çš„æ¨¡å‹")
    logger.error("æœªæ‰¾åˆ°å¯ç”¨çš„ Qwen æ¨¡å‹")
    st.stop()

# åˆå§‹åŒ– Ollama
try:
    llm = Ollama(
        model=model_name,
        temperature=0.7,
        base_url="http://localhost:11434",
        timeout=120
    )
    # æµ‹è¯• LLM æ˜¯å¦æ­£å¸¸å·¥ä½œ
    test_response = llm.invoke("test")
    logger.info(f"Ollama LLM åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
except Exception as e:
    logger.error(f"Ollama LLM åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
    st.error(f"""
    Ollama æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š
    1. Ollama æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ (ollama serve)
    2. {model_name} æ¨¡å‹æ˜¯å¦æ­£ç¡®å®‰è£…
    3. ç«¯å£ 11434 æ˜¯å¦å¯è®¿é—®
    
    é”™è¯¯ä¿¡æ¯: {str(e)}
    """)
    st.stop()

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
    ("human", "{input}")
])
logger.info("æç¤ºæ¨¡æ¿åˆ›å»ºæˆåŠŸ")

# åˆ›å»ºå¤„ç†é“¾
chain = prompt | llm | StrOutputParser()
logger.info("å¤„ç†é“¾åˆ›å»ºæˆåŠŸ")

# æ·»åŠ ä¾§è¾¹æ çŠ¶æ€æ˜¾ç¤º
with st.sidebar:
    st.subheader("ç³»ç»ŸçŠ¶æ€")
    if check_ollama_service():
        st.success("Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
    else:
        st.error("Ollama æœåŠ¡æœªè¿è¡Œ")
    
    # æ·»åŠ æ¸…é™¤æŒ‰é’®
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        logger.info("å¯¹è¯å†å²å·²æ¸…é™¤")
        st.experimental_rerun()

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯æ˜¾ç¤º
    if st.checkbox("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"):
        st.subheader("è°ƒè¯•ä¿¡æ¯")
        st.write("å½“å‰æ¨¡å‹:", llm.model)
        st.write("æœåŠ¡åœ°å€:", llm.base_url)
        st.write("ä¼šè¯æ¶ˆæ¯æ•°:", len(st.session_state.messages))
        
        if st.button("æ£€æŸ¥æœåŠ¡çŠ¶æ€"):
            if check_ollama_service():
                st.success("æœåŠ¡æ­£å¸¸")
            else:
                st.error("æœåŠ¡å¼‚å¸¸")

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥
if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    logger.info(f"æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_input}")
    
    # æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€
    if not check_ollama_service():
        st.error("Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        logger.error("å°è¯•ç”Ÿæˆå›å¤æ—¶å‘ç° Ollama æœåŠ¡æœªè¿è¡Œ")
        st.stop()
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # æ·»åŠ åŠ©æ‰‹å“åº”
    with st.chat_message("assistant"):
        try:
            message_placeholder = st.empty()
            
            # æ·»åŠ çŠ¶æ€æŒ‡ç¤ºå™¨
            with st.status("æ­£åœ¨ç”Ÿæˆå›å¤...", expanded=True) as status:
                st.write("æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...")
                logger.info("å¼€å§‹ç”Ÿæˆå›å¤...")
                
                try:
                    # ç›´æ¥ä½¿ç”¨ invoke è€Œä¸æ˜¯ stream
                    response = chain.invoke({"input": user_input})
                    
                    # æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
                    full_response = ""
                    for char in response:
                        full_response += char
                        message_placeholder.markdown(full_response + "â–Œ")
                        time.sleep(0.01)  # æ·»åŠ å°å»¶è¿Ÿä»¥åˆ›å»ºæ‰“å­—æ•ˆæœ
                    
                    # å®Œæˆåæ›´æ–°æ˜¾ç¤º
                    message_placeholder.markdown(response)
                    status.update(label="å›å¤ç”Ÿæˆå®Œæˆ!", state="complete")
                    logger.info(f"æˆåŠŸç”Ÿæˆå›å¤: {response[:100]}...")
                    
                except Exception as e:
                    status.update(label="ç”Ÿæˆå›å¤å‡ºé”™", state="error")
                    raise e
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.messages.append({"role": "assistant", "content": response})
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg, exc_info=True)
            st.error(error_msg)
            
            # æ·»åŠ é‡è¯•æŒ‰é’®
            if st.button("é‡è¯•"):
                st.experimental_rerun()

# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("powered by Ollama & LangChain ğŸš€") 